---
layout: post
cover: '/assets/images/cover-logs.webp'
navigation: True
title: 'Low-cost Kibana setup that everyone can afford'
date: 2022-10-08 21:00:00  # Blackie's 12th birthday! ‚ù§Ô∏è
tags: [Tips and tricks, Observability, Logs, Kibana, Infrastructure]
subclass: 'post'
author: dominik
categories: dominik
description: 'How to run Kibana cheaply and load the logs in order to read them ad hoc?'
---

Have you ever found yourself in a place where **reading logs feels inefficient**
because your best way of doing this is grepping through plain text? Do you
**miss the experience of Kibana**, which embraces histograms, visualizations, and
filtering, but your working environment does not provide that?

Well, that had been the case for me, but *with a bit of effort, I've managed to
find a good-enough solution*. It bases on running Kibana on your local
machine, so there's no additional server cost for your company. Together with
the setup of ingesting logs from the file, it provides a very flexible way
of running Kibana on demand.

Log storage together with the whole Kibana stack may indeed cost an arm and
a leg, so companies may be hesitant to incorporate it into their infrastructure.
Using the approach described below might be a **quick win for you if you need
Kibana ad hoc**.

#### üìå Prerequisites

- local Kubernetes cluster, I used [minikube](https://github.com/kubernetes/minikube){:target="_blank" rel="noopener noreferrer"}
- [Helm](https://helm.sh/){:target="_blank" rel="noopener noreferrer"} installed

#### üêæ Step-by-step guide

##### Running ELK stack

<ol>
<li>
Clone <a href="https://github.com/elastic/helm-charts" target="_blank" rel="noopener noreferrer">the repo</a>
delivered directly by the Elastic team (thanks for that!).
{% highlight sh %}
git clone git@github.com:elastic/helm-charts.git
{% endhighlight %}
</li>
<li>
Checkout a specific tag, so that all components are compatible with each
other. At the time of writing this post, the newest one is <code>v7.17.3</code>.
{% highlight sh %}
git checkout tags/v7.17.3
{% endhighlight %}
</li>
<li>
Run Elasticsearch, Logstash and Kibana. Commands below will create 
<a href="https://helm.sh/docs/topics/architecture/#the-purpose-of-helm" target="_blank" rel="noopener noreferrer">Helm releases</a>.
{% highlight sh %}
cd elasticsearch/examples/minikube
make install
cd ../../../logstash/examples/elasticsearch
make install
cd ../../../kibana/examples/default
make install
{% endhighlight %}
</li>
<li>
You should be able to see the pods up and running. If instead of
<code>Running</code> status, you see <code>ImagePullBackOff</code>, see below.
{% highlight sh %}
‚ûú  ~ kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
elasticsearch-master-0       1/1     Running   0          100m
elasticsearch-master-1       1/1     Running   0          100m
elasticsearch-master-2       1/1     Running   0          100m
helm-kibana-default-kibana   1/1     Running   0          53s
helm-logstash-elasticsearch  1/1     Running   0          3m51s
{% endhighlight %}
</li>
<ul><li>
What to do if your pods are in <code>ImagePullBackOff</code> status? This means that
downloading Docker image have reached timeout or failed. There can be multiple
reasons for that. You can investigate it further by running:
{% highlight sh %}
kubectl describe pod elasticsearch-master-0
{% endhighlight %}

If the reason is, for example, slow internet connection, you can load the
image manually, as shown below:
{% highlight sh %}
docker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.3
minikube image load docker.elastic.co/elasticsearch/elasticsearch:7.17.3
minikube image list
{% endhighlight %}
</li></ul>
<li>
Enable port forwarding. After that you should be able to access
<a href="http://localhost:5601" target="_blank" rel="noopener noreferrer">http://localhost:5601</a>
and see Kibana interface.
{% highlight sh %}
kubectl port-forward svc/helm-kibana-default-kibana 5601
{% endhighlight %}
</li>
</ol>

Congratulations, you now have the whole ELK stack up and running on your local
machine and you can even load some logs manually from a file in order to get
into play quickly. That was fast and easy, wasn't it?

##### üçº Ingesting automatically from a file

You can already load logs manually, but you may want to utilize more automatic
approach in order to follow them in the real time, as they appear.

<ol>
<li>
The first thing you'll need is a log file that you can follow. I want to read
the logs of my service running in production, so I'll fetch them from Kubernetes
using the <code>--follow</code> flag and pipe them into the file
<code>tmp.log</code> in my home directory.
{% highlight sh %}
kubectl logs --follow deployment/my-service > tmp.log
{% endhighlight %}
</li>
<li>
Having the logs now streamed into the file in my local system, I can try
ingesting them into Elasticsearch. In order to do that,
<a href="https://www.elastic.co/beats/filebeat" target="_blank" rel="noopener noreferrer">Filebeat</a>
seems like a good fit. Let's download and unpack it.
<blockquote>
    <p>
        Please note that I'm using the same version as the rest of the stack:
        <code>7.17.3</code>. You may also need to choose a package that fits
        your operating system in the 
        <a href="https://www.elastic.co/downloads/past-releases/filebeat-7-17-3" target="_blank" rel="noopener noreferrer">download</a> 
        page.
    </p>
</blockquote>
{% highlight sh %}
curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.17.3-darwin-x86_64.tar.gz
tar xzvf filebeat-7.17.3-darwin-x86_64.tar.gz
{% endhighlight %}
</li>
<li>
Then we'll need to tune Filebeat's configuration a little. Enter the directory,
open <code>filebeat.yml</code>, and configure <code>filebeat.inputs</code>, so
that it uses your local log file.
{% highlight sh %}
cd filebeat-7.17.3-darwin-x86_64
vim filebeat.yml
{% endhighlight %}
{% highlight yaml %}
filebeat.inputs:
- type: filestream
  enabled: true
  paths:
  - /Users/dominik/tmp.log
{% endhighlight %}
<blockquote>
    <p>
        Filebeat obviously has plenty other options that can be configured.
        You can find more details in the
        <a href="https://www.elastic.co/guide/en/beats/filebeat/7.17/configuring-howto-filebeat.html" target="_blank" rel="noopener noreferrer">documentation</a>.
    </p>
</blockquote>
</li>
<li>
We're almost ready to go. We yet need to enable port forwarding for Elasticsearch,
so that Filebeat can communicate with it, and then let's start it up!
{% highlight sh %}
kubectl port-forward svc/elasticsearch-master 9200
./filebeat -e
{% endhighlight %}
</li>
</ol>

You'll be able to see Filebeat logs claiming that the connection has been
established and that it periodically scans for new changes. Navigate to
<a href="http://localhost:5601/app/discover" target="_blank" rel="noopener noreferrer">http://localhost:5601/app/discover</a>.
At first, you'll need to create an index pattern. Just type <code>filebeat*</code>
and return to the Discover page.

You should be able to see your logs, congratulations! You now have a powerful
tool in hand and it didn't require much time nor a complex setup.

##### üß™ Further tuning

In order to utilize the advantages that Kibana has to offer, you may want to
use **ingest pipelines**, which allow you to parse your logs into individual fields
that you can use for filtering and searching.

It's very simple if you already have one created, because
in such case the only thing you need to do is to set its name in
<code>filebeat.yml</code> under
<a href="https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-input-filestream.html#_pipeline_6" target="_blank" rel="noopener noreferrer"><code>filebeat.inputs[].pipeline</code></a>
property.

If you don't have any and you don't know how to create one, check out the way of
<a href="http://localhost:5601/app/home#/tutorial_directory/fileDataViz" target="_blank" rel="noopener noreferrer">uploading files manually</a>.
It provides a feature that **automatically detects your log format** and tries to
create an ingest pipeline for them. It's not ideal, but may be good enough for
your needs.

##### üßπ Cleaning up

When you're done, you can stop the whole stack by running
<code>helm uninstall</code> for all three Helm releases that you've created.
You can see their names by running <code>helm list</code>.

Alternatively, you can stop the whole minikube cluster by running
<code>minikube stop</code>, but pods will start once you start the cluster
again.

üéâ That's it, **enjoy reading your logs**! And let me know if it worked for you!
